{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new representation of a deep neural network \n",
    "#### Using the example of VGG-16 (K. Simonyan, A. Zisserman, Very Deep Convolutional Networks for Large-Scale Image Recognition)\n",
    "\n",
    "In this tutorial we build a representation of the VGG-16 convolutional neural network that can be used to predict the execution time of the model on different hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mlpredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new instance of the dnn class and add layers to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_1 (Convolution), now 224x224 with 64 channels\n",
      "conv1_2 (Convolution), now 224x224 with 64 channels\n",
      "pool1 (Max_pool), now 112x112 with 64 channels\n",
      "conv2_1 (Convolution), now 112x112 with 128 channels\n",
      "conv2_2 (Convolution), now 112x112 with 128 channels\n",
      "pool2 (Max_pool), now 56x56 with 128 channels\n",
      "conv3_1 (Convolution), now 56x56 with 256 channels\n",
      "conv3_2 (Convolution), now 56x56 with 256 channels\n",
      "conv3_3 (Convolution), now 56x56 with 256 channels\n",
      "pool3 (Max_pool), now 28x28 with 256 channels\n",
      "conv4_1 (Convolution), now 28x28 with 512 channels\n",
      "conv4_2 (Convolution), now 28x28 with 512 channels\n",
      "conv4_3 (Convolution), now 28x28 with 512 channels\n",
      "pool4 (Max_pool), now 14x14 with 512 channels\n",
      "conv5_1 (Convolution), now 14x14 with 512 channels\n",
      "conv5_2 (Convolution), now 14x14 with 512 channels\n",
      "conv5_3 (Convolution), now 14x14 with 512 channels\n",
      "pool5 (Max_pool), now 7x7 with 512 channels\n",
      "fc6 (Convolution), now 1x1 with 4096 channels\n",
      "fc7 (Convolution), now 1x1 with 4096 channels\n",
      "fc8 (Convolution), now 1x1 with 1000 channels\n"
     ]
    }
   ],
   "source": [
    "VGG16 = mlpredict.api.Dnn(input_dimension=3, input_size=224)\n",
    "\n",
    "\n",
    "VGG16.add_layer('Convolution', 'conv1_1', kernelsize=3, channels_out=64, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Convolution', 'conv1_2', kernelsize=3, channels_out=64, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Max_pool', 'pool1', pool_size=2, padding='SAME', strides=2)\n",
    "\n",
    "\n",
    "VGG16.add_layer('Convolution', 'conv2_1', kernelsize=3, channels_out=128, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Convolution', 'conv2_2', kernelsize=3, channels_out=128, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Max_pool', 'pool2', pool_size=2, padding='SAME', strides=2)\n",
    "\n",
    "\n",
    "VGG16.add_layer('Convolution', 'conv3_1', kernelsize=3, channels_out=256, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Convolution', 'conv3_2', kernelsize=3, channels_out=256, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Convolution', 'conv3_3', kernelsize=3, channels_out=256, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Max_pool', 'pool3', pool_size=2, padding='SAME', strides=2)\n",
    "\n",
    "\n",
    "VGG16.add_layer('Convolution', 'conv4_1', kernelsize=3, channels_out=512, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Convolution', 'conv4_2', kernelsize=3, channels_out=512, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Convolution', 'conv4_3', kernelsize=3, channels_out=512, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Max_pool', 'pool4', pool_size=2, padding='SAME', strides=2)\n",
    "\n",
    "\n",
    "VGG16.add_layer('Convolution', 'conv5_1', kernelsize=3, channels_out=512, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Convolution', 'conv5_2', kernelsize=3, channels_out=512, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Convolution', 'conv5_3', kernelsize=3, channels_out=512, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Max_pool', 'pool5', pool_size=2, padding='SAME', strides=2)\n",
    "\n",
    "\n",
    "VGG16.add_layer('Convolution', 'fc6', kernelsize=7, channels_out=4096, \n",
    "                padding='VALID', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Convolution', 'fc7', kernelsize=1, channels_out=4096, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')\n",
    "VGG16.add_layer('Convolution', 'fc8', kernelsize=1, channels_out=1000, \n",
    "                padding='SAME', strides=1, use_bias=1, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 layer network\n",
      "\n",
      "Input size 224x224x3\n",
      "\n",
      "conv1_1 (Convolution), now 224x224 with 64 channels\n",
      "conv1_2 (Convolution), now 224x224 with 64 channels\n",
      "pool1 (Max_pool), now 112x112 with 64 channels\n",
      "conv2_1 (Convolution), now 112x112 with 128 channels\n",
      "conv2_2 (Convolution), now 112x112 with 128 channels\n",
      "pool2 (Max_pool), now 56x56 with 128 channels\n",
      "conv3_1 (Convolution), now 56x56 with 256 channels\n",
      "conv3_2 (Convolution), now 56x56 with 256 channels\n",
      "conv3_3 (Convolution), now 56x56 with 256 channels\n",
      "pool3 (Max_pool), now 28x28 with 256 channels\n",
      "conv4_1 (Convolution), now 28x28 with 512 channels\n",
      "conv4_2 (Convolution), now 28x28 with 512 channels\n",
      "conv4_3 (Convolution), now 28x28 with 512 channels\n",
      "pool4 (Max_pool), now 14x14 with 512 channels\n",
      "conv5_1 (Convolution), now 14x14 with 512 channels\n",
      "conv5_2 (Convolution), now 14x14 with 512 channels\n",
      "conv5_3 (Convolution), now 14x14 with 512 channels\n",
      "pool5 (Max_pool), now 7x7 with 512 channels\n",
      "fc6 (Convolution), now 1x1 with 4096 channels\n",
      "fc7 (Convolution), now 1x1 with 4096 channels\n",
      "fc8 (Convolution), now 1x1 with 1000 channels\n"
     ]
    }
   ],
   "source": [
    "VGG16.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the representation of the VGG-16 deep neural network as json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16.save('models/VGG16.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
